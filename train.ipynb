{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm # ProgressBar for loops\n",
    "                                                                                                            \n",
    "from tensorflow.python.ops import rnn_cell, seq2seq\n",
    "from utils.data_loader import SKTDataLoader\n",
    "\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers = 3   # Number of layers of RNN\n",
    "num_hidden = 128  # Hidden size of RNN cell\n",
    "batch_size = 64  # Number of sentences in a batch\n",
    "seq_length = 50  # Length of sequence\n",
    "split = [0.90, 0.05, 0.05] # Splitting proportions into train, valid, test\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "verbose = 1      # Display every <verbose> epochs\n",
    "\n",
    "model_name = 'skt_3_1000' # Name is <skt>_<num_layers>_<sentencepiece_vocabsize>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded complete data\n",
      "Loaded vocab\n",
      "Loaded word2idx and idx2word\n",
      "Created dataset\n",
      "Vocab Size: 1797\n",
      "Data Size: 434969\n",
      "391472 21748 21749\n"
     ]
    }
   ],
   "source": [
    "data_loader = SKTDataLoader('data/input_complete_1000_split.txt','data/output_complete_1000_split.txt',batch_size,seq_length, split=split)\n",
    "vocab_size =  data_loader.vocab_size   # Number of unique words in dataset\n",
    "\n",
    "data_size = data_loader.data_size      # Number of paris in the entire dataset\n",
    "train_set_size = data_loader.train_size# Number of pairs in train set\n",
    "valid_set_size = data_loader.valid_size# Number of pairs in valid set\n",
    "test_set_size = data_loader.test_size  # Number of pairs in test set\n",
    "\n",
    "num_train_batches = int(train_set_size*1.0/batch_size) # Number of train batches1\n",
    "num_valid_batches = int(valid_set_size*1.0/batch_size)\n",
    "num_test_batches = int(test_set_size*1.0/batch_size)\n",
    "\n",
    "print \"Vocab Size: \" + str(vocab_size)\n",
    "print \"Data Size: \" + str(data_size)\n",
    "print train_set_size, valid_set_size, test_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('encode_input'):\n",
    "    encode_input = [tf.placeholder(tf.int32, shape=(None,), name = \"ei_%i\" %i) for i in range(seq_length)]\n",
    "\n",
    "with tf.name_scope('labels'):\n",
    "    labels = [tf.placeholder(tf.int32, shape=(None,), name = \"l_%i\" %i) for i in range(seq_length)]\n",
    "\n",
    "with tf.name_scope('decode_input'):\n",
    "    decode_input = [tf.zeros_like(encode_input[0], dtype=np.int32, name=\"GO\")] + labels[:-1]\n",
    "    \n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(\"float\", name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x6714a50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0xcbaadd0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0xcbaac90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "cells = [rnn_cell.DropoutWrapper(\n",
    "        rnn_cell.BasicLSTMCell(num_hidden), output_keep_prob=keep_prob\n",
    "    ) for i in range(num_layers)]\n",
    "\n",
    "stacked_lstm = rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "with tf.variable_scope(\"decoders\") as scope:\n",
    "    decode_outputs, decode_state = seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, stacked_lstm, vocab_size, vocab_size, embedding_size=num_hidden)\n",
    "    \n",
    "#     decode_outputs, decode_state = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "#         encode_input, decode_input, stacked_lstm, vocab_size, vocab_size, num_hidden)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "#     decode_outputs, decode_state = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "#         encode_input, decode_input, stacked_lstm, vocab_size, vocab_size, num_hidden, feed_previous=True)\n",
    "    \n",
    "    decode_outputs_test, decode_state_test = seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, stacked_lstm,vocab_size,vocab_size,embedding_size=num_hidden,feed_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ScalarSummary:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in labels]\n",
    "    loss = seq2seq.sequence_loss(decode_outputs, labels, loss_weights, vocab_size)\n",
    "\n",
    "#with tf.name_scope('accuracy'):\n",
    "    #macro_accuracy = \n",
    "\n",
    "tf.scalar_summary('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "#sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "merged = tf.merge_all_summaries()\n",
    "summary_writer = tf.train.SummaryWriter('logs/' + model_name , sess.graph)\n",
    "\n",
    "#saver.restore(sess, 'models/' + model_name)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6116/6116 [35:30<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Steps: 6116 train_loss: 1.2375+0.29 valid_loss: 0.871+0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6116/6116 [34:52<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Steps: 12232 train_loss: 0.6605+0.11 valid_loss: 0.5177+0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6116/6116 [35:05<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Steps: 18348 train_loss: 0.4368+0.06 valid_loss: 0.3803+0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6116/6116 [35:27<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Steps: 24464 train_loss: 0.339+0.05 valid_loss: 0.315+0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6116/6116 [34:57<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Steps: 30580 train_loss: 0.2843+0.04 valid_loss: 0.2763+0.04\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        # Training on train set\n",
    "        for i in tqdm(range(num_train_batches)):\n",
    "            batch_inp, batch_outp = data_loader.next_batch()\n",
    "\n",
    "            input_dict = {encode_input[t]: batch_inp[t] for t in range(seq_length)}\n",
    "            input_dict.update({labels[t]: batch_outp[t] for t in range(seq_length)})\n",
    "            input_dict[keep_prob] = 1.0\n",
    "\n",
    "            _, loss_val, summary = sess.run([train, loss, merged], feed_dict=input_dict)\n",
    "            train_losses.append(loss_val)\n",
    "\n",
    "            summary_writer.add_summary(summary, step)\n",
    "            step += 1\n",
    "\n",
    "        # Testing on valid set\n",
    "        for i in range(num_valid_batches):\n",
    "            batch_inp, batch_outp = data_loader.next_batch(data_type='valid')\n",
    "\n",
    "            input_dict = {encode_input[t]: batch_inp[t] for t in range(seq_length)}\n",
    "            input_dict.update({labels[t]: batch_outp[t] for t in range(seq_length)})\n",
    "            input_dict[keep_prob] = 1.0\n",
    "\n",
    "            loss_val = sess.run(loss, feed_dict=input_dict)\n",
    "            valid_losses.append(loss_val)\n",
    "\n",
    "        if epoch % verbose == 0:\n",
    "            log_txt = \"Epoch: \" + str(epoch) + \" Steps: \" + str(step) + \\\n",
    "                \" train_loss: \" + str(round(np.mean(train_losses),4)) + '+' + str(round(np.std(train_losses),2)) + \\\n",
    "                \" valid_loss: \" + str(round(np.mean(valid_losses),4)) + '+' + str(round(np.std(valid_losses),2)) \n",
    "            print log_txt\n",
    "\n",
    "            f = open('log.txt', 'a')\n",
    "            f.write(log_txt + '\\n')\n",
    "            f.close()\n",
    "\n",
    "            saver.save(sess, 'models/' + model_name)\n",
    "except KeyboardInterrupt:\n",
    "    print \"Stopped at epoch: \" + str(epoch) + ' and step: ' + str(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 0.2746+0.04\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "\n",
    "# Testing on test set\n",
    "for i in range(num_test_batches):\n",
    "    batch_inp, batch_outp = data_loader.next_batch(data_type='test')\n",
    "\n",
    "    input_dict = {encode_input[t]: batch_inp[t] for t in range(seq_length)}\n",
    "    input_dict.update({labels[t]: batch_outp[t] for t in range(seq_length)})\n",
    "    input_dict[keep_prob] = 1.0\n",
    "\n",
    "    loss_val = sess.run(loss, feed_dict=input_dict)\n",
    "    test_losses.append(loss_val)\n",
    "\n",
    "\n",
    "log_txt = \"Test_loss: \" + str(round(np.mean(test_losses), 4)) + '+' + str(round(np.std(test_losses), 2)) \n",
    "print log_txt\n",
    "\n",
    "f = open('log.txt', 'a')\n",
    "f.write(log_txt + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting output and analysing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get batch data like batch_inp, batch_outp\n",
    "2. get outputs from decode_outputs_test and argmax\n",
    "3. use swapaxes to change it to [batch_size, seq_length] for all batch_inp, batch_outp, output\n",
    "4. use data_loader.idx2word to generate words for all batch_inp, batch_outp, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a3b5c8da86bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mencode_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_inp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_outp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/du1/13CS30036/vikas/skt/utils/data_loader.py\u001b[0m in \u001b[0;36mrandom_batch\u001b[0;34m(self, data_type)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrandom_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0mtemp_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "batch_inp, batch_outp = data_loader.random_batch()\n",
    "\n",
    "input_dict = {encode_input[t]: batch_inp[t] for t in range(seq_length)}\n",
    "input_dict.update({labels[t]: batch_outp[t] for t in range(seq_length)})\n",
    "input_dict[keep_prob] = 1.0\n",
    "\n",
    "loss_val, outputs = sess.run([loss, decode_outputs_test], feed_dict = input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded_outputs = np.array(outputs).transpose([1,0,2])\n",
    "decoded_outputs = np.argmax(outputs, axis = 2)\n",
    "\n",
    "inps = np.swapaxes(batch_inp, 0, 1)\n",
    "outps = np.swapaxes(batch_outp, 0, 1)\n",
    "gens = np.swapaxes(decoded_outputs, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁na▁ca▁prIRayate▁cakzuH▁sadA▁duryoDanasya▁saH\n",
      "▁na▁ca▁prIRay▁cakzus▁sadA▁duryoDana▁tad\n",
      "U▁pra▁pra▁pra▁praas1616161616cRaRaRaRaRaRaRaRaRaIIIIIahahah202020alalalalalalalalalalalalalalalalal▁B\n",
      "\n",
      "▁uBAByAM▁mucyate▁yogI▁tatrAnandamayo▁Bavet\n",
      "▁uB▁muc▁yogin▁tatra▁Ananda▁maya▁BU\n",
      "52Prururu::maccccccc????asasas▁kf▁kf▁kf▁kf▁kf▁kfrṝṝṝṝṝṝṝ▁sa▁sa▁sa▁sa▁sa▁sa▁sa▁sayaMyaMiiiiadad\n",
      "\n",
      "▁prajapet▁parameSAni▁prAsAdAKyaM▁mahAmanum\n",
      "▁prajap▁parameSAnI▁prAsAda▁AKyA▁mahA▁manu\n",
      "kaQQQQ41616̨̨̨̨̨̨̨̨̨̨aHaHaHaHaHaHaH▁a▁a▁a▁pra▁pra▁pra▁pra▁pra▁prariririririririririri66666v\n",
      "\n",
      "▁rakzasAM▁BImavegAnAM▁samarezv▁anivartinAm\n",
      "▁rakzas▁BIma▁vega▁samara▁anivartin\n",
      "3▁p▁p▁p▁p▁ptttyoyoyoyoyoyoyoyoyommmmmmmmDDDDDDD????????amamamamamamamamamamamamam\n",
      "\n",
      "▁kAryo'nyatra▁pratijYAyAH▁prayogo▁na▁kaTaMcana\n",
      "▁kf▁anyatra▁pratijYA▁prayoga▁na▁kaTaMcana\n",
      "20yaMyaMyaMyaMyaMyaMyaMaaa888alalalalalalalalalalal▁B▁B▁B▁B▁B▁B▁B▁B▁B▁B▁B▁B▁B▁B▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka\n",
      "\n",
      "▁DArozRaM▁gopayaH▁peyaM▁mAtramanu▁priye\n",
      "▁DArozRa▁go▁payas▁pA▁mAtra▁anu▁priya\n",
      "Am▁p▁p▁pCCCCCCCCU▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka\n",
      "\n",
      "▁baBUva▁paramakrudDo▁rAvaRo▁BImadarSanaH\n",
      "▁BU▁parama▁kruD▁rAvaRa▁BIma▁darSana\n",
      ".kzkzkzkzkzkzRaRaRaRaRaRaRaasyaasyaasyaasyaasyaasyaasyą̨̨5555555AdAdAdAdAdAd?cacacacacaTaTaTaTaTaTaTa\n",
      "\n",
      "▁rAjaputri▁mahABAge▁fzipatni▁varAnane\n",
      "▁rAjaputrI▁mahABAga▁fzi▁patnI▁varAnanA\n",
      "16161616HHHHHHHHahahKKK▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka▁ka\n",
      "\n",
      "▁AdityASca▁taTA▁rAjan▁rudrASca▁vasavo▁'SvinO\n",
      "▁Aditya▁ca▁taTA▁rAjan▁rudra▁ca▁vasu▁aSvin\n",
      "▁p▁p▁tu▁tu▁tu▁tu▁tu▁tu▁tu▁tu▁tu▁tu▁tu▁tu▁tu▁tu333331717atatatatatatatatatatatathahahahahauuuuuuuu4040\n",
      "\n",
      "▁jyezWaM▁kuntIsutaM▁jAtaM▁SrutvA▁ravisamapraBam\n",
      "▁jyezWa▁kuntI▁suta▁jan▁Sru▁ravi▁sama▁praBA\n",
      "vavavavavava▁pra▁pra▁pra▁pra▁pra▁prarrrrrrahahahahah202020▁a▁a▁a▁a▁a▁a▁a▁a????????oobbb▁vi▁vi▁vi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#index = random.randint(0, batch_size-1)\n",
    "for index in range(10):\n",
    "    inp = ''.join([data_loader.idx2word[x] for x in inps[index] if x != vocab_size-1][::-1])#.replace('\\xe2\\x96\\x81', ' ')\n",
    "    outp = ''.join([data_loader.idx2word[x] for x in outps[index] if x != vocab_size-1])#.replace('\\xe2\\x96\\x81', ' ')\n",
    "    gen = ''.join([data_loader.idx2word[x] for x in gens[index] if x != vocab_size-1])#.replace('\\xe2\\x96\\x81', ' ')\n",
    "    \n",
    "    inp_raw = inps[index]\n",
    "    outp_raw = outps[index]\n",
    "    gen_raw = gens[index]\n",
    "    \n",
    "    \n",
    "    print inp\n",
    "    print outp\n",
    "    print gen\n",
    "    \n",
    "#     print [x for x in inp_raw[::-1] if x != vocab_size-1]\n",
    "#     print [x for x in outp_raw if x != vocab_size-1]\n",
    "#     print [x for x in gen_raw if x != vocab_size-1]\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputting the previous word for decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_inp, batch_outp = data_loader.random_batch()\n",
    "\n",
    "input_dict = {encode_input[t]: batch_inp[t] for t in range(seq_length)}\n",
    "input_dict.update({labels[t]: batch_outp[t] for t in range(seq_length)})\n",
    "input_dict[keep_prob] = 1.0\n",
    "\n",
    "#loss_val, outputs = sess.run([loss, decode_outputs_test], feed_dict = input_dict)\n",
    "loss_val, outputs = sess.run([loss, decode_outputs], feed_dict = input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded_outputs = np.array(outputs).transpose([1,0,2])\n",
    "decoded_outputs = np.argmax(outputs, axis = 2)\n",
    "\n",
    "inps = np.swapaxes(batch_inp, 0, 1)\n",
    "outps = np.swapaxes(batch_outp, 0, 1)\n",
    "gens = np.swapaxes(decoded_outputs, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁suvarRamaRimuktAQye▁jAyate▁vipule▁kule\n",
      "lauk▁laup▁vinja▁yaQA▁Atkum▁iRma▁Rarvau▁s\n",
      "mamamamamamamamamamamamamama▁B▁B▁B▁B▁B▁B▁B▁B▁B▁B▁B▁BmamamamamamamaJ]J]J]bbbbbb▁mad▁mad▁mad▁mad▁madkzkz▁\n",
      "\n",
      "▁svayaM▁pitrA▁svareRoccEstrIMllokAn▁anunAdya▁vE\n",
      "Ev▁ydaAnun▁akaol▁rit▁sEccu▁rava▁sfti▁pmyava▁s\n",
      "mamamamamamamamaad77777ye7777vvvvvvvyAyAyAyAyAyAyAyAyAyACCCCCCCC\"\"\"sasasa\n",
      "\n",
      "▁vizAdo▁vizadaScEva▁vidyutaH▁kAntakaH▁praBuH\n",
      "uB▁prakatanAk▁tauyd▁vi▁eva▁cadaza▁vidaAz▁vi\n",
      "mamamamamamamamamamamamamamamamamamamamama848484±±±±±±▁B▁B▁B▁B▁B▁B▁B▁B▁B▁B▁B▁B▁B\"\"\"vAvA..\n",
      "\n",
      "▁maheSvaraH▁praYcavakro▁daSabAhurvfzaDvajaH\n",
      "javaDzafv▁uhAbSada▁ravaSehma▁\n",
      "mamamamamamamamamamamamatitititititiṝ±2ṝ±2ṝ±2ṝ±2ṝ±2oooooooddddd▁tu▁tu▁tu▁tu▁tu▁tu▁tu▁tu▁tu333...\n",
      "\n",
      "▁BUtAni▁prakftiScEva▁indriyARi▁ca▁sarvaSaH\n",
      "sSavar▁sa▁cayaridni▁▁eva▁catifk▁prata▁BU\n",
      "mamamamamamamamamamamamamamamamama▁BU▁BU▁BU▁BU▁BUtata▁ka▁ka▁ka▁ka▁ka▁katata▁ka▁ka▁ka▁kaSaSaSa'''''':::::\n",
      "\n",
      "▁aTorDvaM▁mAMsavargAnupadekzyAmaH\n",
      "Sidpau▁garva▁saMAm▁mvaDrU▁Ta▁a\n",
      "mamamamamamamamamamatititi▁BU▁BU▁BU▁BU▁BU▁BU▁BU▁BU▁BU▁BU▁BU▁BU????????KKKKKKKKKKKKKKKKK\n",
      "\n",
      "▁uzRaM▁vAte▁kaPe▁toyaM▁pitte▁rakte▁ca▁SItalam\n",
      "lataI▁S▁catakra▁tati▁pyaot▁aP▁kataAv▁Razu▁\n",
      "mamamamamamatitititititiwwJ]J]J]▁a▁ayeyeye▁annnnnnn▁tu▁tu▁tuahahahalalalal▁tu▁tu'''KKKKK\n",
      "\n",
      "▁aBizekakftas▁tatra▁niyato▁niyatASanaH\n",
      "naSa▁amya▁nimya▁nitrata▁▁kfkaeziB▁a\n",
      "mamamamamamamamamamamamamamamamaSSSS▁BU▁BU6464646464yayayayayayayayayayayayayayayayayaya¡¡¡¡r\n",
      "\n",
      "▁pUrvoktaviDinA▁yukte▁sarvalakzaRasaMyute\n",
      "tauy▁saMRazakla▁var▁sajuy▁iD▁vicva▁varU▁p\n",
      "mamamamamamamamatititititititi88▁a▁a▁a▁a▁a2020▁ka▁ka20202020202020202020202020202020UUUUUUNra\n",
      "\n",
      "▁yato▁nirvizayasyAsya▁manaso▁muktir▁izyate\n",
      "zi▁tikum▁snama▁mdai▁yazavir▁nistaya▁\n",
      "mamamamamamamamamamamamamamamamamamayayayayayayayayayayayayayayayayayayayayayayayayaya▁b▁bya¡¡aMP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#index = random.randint(0, batch_size-1)\n",
    "for index in range(10):\n",
    "    inp = ''.join([data_loader.idx2word[x] for x in inps[index] if x != vocab_size-1][::-1])#.replace('\\xe2\\x96\\x81', ' ')\n",
    "    outp = ''.join([data_loader.idx2word[x] for x in outps[index] if x != vocab_size-1][::-1])#.replace('\\xe2\\x96\\x81', ' ')\n",
    "    gen = ''.join([data_loader.idx2word[x] for x in gens[index] if x != vocab_size-1][::-1])#.replace('\\xe2\\x96\\x81', ' ')\n",
    "    \n",
    "    print inp\n",
    "    print outp\n",
    "    print gen\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Checking for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_inp, batch_outp = data_loader.random_batch(data_type='test')\n",
    "\n",
    "input_dict = {encode_input[t]: batch_inp[t] for t in range(seq_length)}\n",
    "input_dict.update({labels[t]: batch_outp[t] for t in range(seq_length)})\n",
    "input_dict[keep_prob] = 1.0\n",
    "\n",
    "#loss_val, outputs = sess.run([loss, decode_outputs_test], feed_dict = input_dict)\n",
    "loss_val, outputs = sess.run([loss, decode_outputs], feed_dict = input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded_outputs = np.array(outputs).transpose([1,0,2])\n",
    "decoded_outputs = np.argmax(outputs, axis = 2)\n",
    "\n",
    "inps = np.swapaxes(batch_inp, 0, 1)\n",
    "outps = np.swapaxes(batch_outp, 0, 1)\n",
    "gens = np.swapaxes(decoded_outputs, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁anena▁kAraRenAham▁iha▁vAsaM▁na▁rocaye\n",
      "▁idam▁kAraRa▁mad▁iha▁vAsa▁na▁rocay\n",
      "▁idam▁kAraRa▁mad▁hha▁vAsana▁rocay\n",
      "\n",
      "▁sarve▁kzayAntA▁nicayAH▁patanAntAH▁samucCrayAH\n",
      "▁sarva▁kzaya▁anta▁nicaya▁patana▁anta▁samucCraya\n",
      "▁sarva▁kzaya▁ninta▁niS▁pa▁patana▁anta▁smucCraya\n",
      "\n",
      "▁aByAsAt▁sADakaH▁strIRAM▁SataM▁jayati▁nityaSaH\n",
      "▁aByAsa▁sADaka▁strI▁Sata▁ji▁nityaSas\n",
      "▁aByAsa▁sADay▁strI▁Sata▁ji▁nityaSas\n",
      "\n",
      "▁taTAstu▁Badre▁pOtras▁te▁kzatrAcAro▁Bavet▁Druvam\n",
      "▁taTA▁as▁Badra▁pOtra▁tvad▁kzatra▁AcAra▁BU▁Druvam\n",
      "▁taTA▁tads▁Badra▁pOtra▁tfd▁kAtra▁AcAra▁BU▁Druvam\n",
      "\n",
      "▁vajrasaMhananAH▁sarve▁sarve▁cOGabalAs▁taTA\n",
      "▁vajra▁saMhanana▁sarva▁sarva▁ca▁oGa▁bala▁taTA\n",
      "▁vajra▁saMhanna▁sarva▁sarva▁▁hGa▁bala▁taTA\n",
      "\n",
      "▁te▁SarA▁nataparvARo▁viviSU▁rAkzasaM▁tadA\n",
      "▁tad▁Sara▁nam▁parvan▁viS▁rAkzasa▁tadA\n",
      "▁tad▁Sara▁naraparvan▁viSvvkzasa▁A\n",
      "\n",
      "▁astArkasamavarRaM▁ca▁lakzyate▁jIvato▁yaTA\n",
      "▁asta▁arka▁sama▁varRa▁ca▁lakzay▁jIv▁yaTA\n",
      "▁asArArka▁sarvavarRa▁ca▁lajzay▁jYv▁taTA\n",
      "\n",
      "▁nimittam▁aprayojakaM▁prakftInAM▁varaRaBedas▁tu▁tataH▁kzetrikavat\n",
      "▁nimitta▁aprayojaka▁prakfti▁varaRa▁Beda▁tu▁tatas▁kzetrika▁vat\n",
      "▁nimitta▁prajaojaka▁prakf▁▁var▁▁Beda▁▁tatas▁kzitrarika▁vt\n",
      "\n",
      "▁kIwaH▁peSaskftaM▁DyAyan▁kuqyAM▁tena▁praveSitaH\n",
      "▁kIwa▁peSaskft▁DyA▁kuqI▁tad▁praveSay\n",
      "▁kIwa▁pIza▁sff▁sDyA▁kuqI▁tad▁praveSay\n",
      "\n",
      "▁aSrOzaM▁rAGavasyAhaM▁seyam▁AsAditA▁mayA\n",
      "▁Sru▁rAGava▁mad▁tad▁idam▁AsAday▁mad\n",
      "▁aru▁eGa▁eva▁mad▁▁aAdam▁AhAhay▁mad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#index = random.randint(0, batch_size-1)\n",
    "for index in range(10):\n",
    "    inp = ''.join([data_loader.idx2word[x] for x in inps[index] if x != vocab_size-1][::-1])#.replace('\\xe2\\x96\\x81', ' ')\n",
    "    outp = ''.join([data_loader.idx2word[x] for x in outps[index] if x != vocab_size-1])#.replace('\\xe2\\x96\\x81', ' ')\n",
    "    gen = ''.join([data_loader.idx2word[x] for x in gens[index] if x != vocab_size-1])#.replace('\\xe2\\x96\\x81', ' ')\n",
    "    \n",
    "    inp_raw = inps[index]\n",
    "    outp_raw = outps[index]\n",
    "    gen_raw = gens[index]\n",
    "       \n",
    "    print inp\n",
    "    print outp\n",
    "    print gen\n",
    "    \n",
    "#     print [x for x in inp_raw[::-1] if x != vocab_size-1]\n",
    "#     print [x for x in outp_raw if x != vocab_size-1]\n",
    "#     print [x for x in gen_raw if x != vocab_size-1]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculating precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting outputs on entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader.reset_index(data_type='test')\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "y_out = []\n",
    "\n",
    "for i in tqdm(range(num_test_batches)):\n",
    "    \n",
    "    batch_inp, batch_outp = data_loader.next_batch(data_type='test')\n",
    "\n",
    "    input_dict = {encode_input[t]: batch_inp[t] for t in range(seq_length)}\n",
    "    input_dict.update({labels[t]: batch_outp[t] for t in range(seq_length)})\n",
    "    input_dict[keep_prob] = 1.0\n",
    "\n",
    "    loss_val, outputs = sess.run([loss, decode_outputs_test], feed_dict = input_dict)\n",
    "\n",
    "    decoded_outputs = np.array(outputs).transpose([1,0,2])\n",
    "    decoded_outputs = np.argmax(outputs, axis = 2)\n",
    "\n",
    "    inps = np.swapaxes(batch_inp, 0, 1)\n",
    "    outps = np.swapaxes(batch_outp, 0, 1)\n",
    "    gens = np.swapaxes(decoded_outputs, 0, 1)\n",
    "\n",
    "    for index in range(batch_size):\n",
    "        inp = ''.join([data_loader.idx2word[x] for x in inps[index] if x != vocab_size-1][::-1])\n",
    "        outp = ''.join([data_loader.idx2word[x] for x in outps[index] if x != vocab_size-1])\n",
    "        gen = ''.join([data_loader.idx2word[x] for x in gens[index] if x != vocab_size-1])\n",
    "\n",
    "        X_test.append(inp.split())\n",
    "        y_test.append(outp.split())\n",
    "        y_out.append(gen.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for outp, gen in zip(y_test, y_out):\n",
    "    intersection = set(outp).intersection(gen)\n",
    "    prec = intersection*1.0/len(gen)\n",
    "    recall = intersection*1.0/len(outp)\n",
    "    \n",
    "    precisions.append(prec)\n",
    "    recalls.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_prec = np.mean(precisions)*100.0\n",
    "avg_recall = np.mean(recalls)*100.0\n",
    "f1_score = 2*avg_prec*avg_recall/(avg_prec + avg_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Precision: \" + str(avg_prec) \n",
    "print \"Recall: \" + str(avg_recall)\n",
    "print \"F1_score: \" + str(f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
