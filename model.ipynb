{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm # ProgressBar for loops\n",
    "                                                                                                            \n",
    "from tensorflow.python.ops import rnn_cell, seq2seq\n",
    "from utils.data_loader import SKTDataLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers = 1   # Number of layers of RNN\n",
    "num_hidden = 128 # Hidden size of RNN cell\n",
    "batch_size = 64 \n",
    "seq_length = 150  # Length of sequence\n",
    "num_outputs = 2  \n",
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "\n",
    "model_name = 'skt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded complete data\n",
      "Loaded vocab\n",
      "Loaded word2idx and idx2word\n",
      "Created dataset\n",
      "Vocab Size: 14682\n",
      "Data Size: 441611\n",
      "309127 66242 66242\n"
     ]
    }
   ],
   "source": [
    "data_loader = SKTDataLoader('data/input_complete_split.txt', 'data/output_complete_split.txt',batch_size,seq_length)\n",
    "vocab_size =  data_loader.vocab_size   # Number of unique words in dataset\n",
    "\n",
    "data_size = data_loader.data_size      # Number of paris in the entire dataset\n",
    "train_set_size = data_loader.train_size# Number of pairs in train set\n",
    "valid_set_size = data_loader.valid_size# Number of pairs in valid set\n",
    "test_set_size = data_loader.test_size  # Number of pairs in test set\n",
    "\n",
    "num_train_batches = int(train_set_size*1.0/batch_size) # Number of train batches1\n",
    "num_valid_batches = int(valid_set_size*1.0/batch_size)\n",
    "num_test_batches = int(test_set_size*1.0/batch_size)\n",
    "\n",
    "print \"Vocab Size: \" + str(vocab_size)\n",
    "print \"Data Size: \" + str(data_size)\n",
    "print train_set_size, valid_set_size, test_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('encode_input'):\n",
    "    encode_input = [tf.placeholder(tf.int32, shape=(None,), name = \"ei_%i\" %i) for i in range(seq_length)]\n",
    "\n",
    "with tf.name_scope('labels'):\n",
    "    labels = [tf.placeholder(tf.int32, shape=(None,), name = \"l_%i\" %i) for i in range(seq_length)]\n",
    "\n",
    "with tf.name_scope('decode_input'):\n",
    "    decode_input = [tf.zeros_like(encode_input[0], dtype=np.int32, name=\"GO\")] + labels[:-1]\n",
    "    \n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(\"float\", name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x63fe1d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "cells = [rnn_cell.DropoutWrapper(\n",
    "        rnn_cell.BasicLSTMCell(num_hidden), output_keep_prob=keep_prob\n",
    "    ) for i in range(num_layers)]\n",
    "\n",
    "stacked_lstm = rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "with tf.variable_scope(\"decoders\") as scope:\n",
    "    decode_outputs, decode_state = seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, stacked_lstm, vocab_size, vocab_size, 128)\n",
    "    \n",
    "#     decode_outputs, decode_state = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "#         encode_input, decode_input, stacked_lstm, vocab_size, vocab_size, num_hidden)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "#     decode_outputs, decode_state = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "#         encode_input, decode_input, stacked_lstm, vocab_size, vocab_size, num_hidden, feed_previous=True)\n",
    "    \n",
    "    decode_outputs_test, decode_state_test = seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, stacked_lstm, vocab_size, vocab_size, 128, feed_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ScalarSummary:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in labels]\n",
    "    loss = seq2seq.sequence_loss(decode_outputs, labels, loss_weights, vocab_size)\n",
    "\n",
    "tf.scalar_summary('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "#sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "merged = tf.merge_all_summaries()\n",
    "summary_writer = tf.train.SummaryWriter('logs/' + model_name , sess.graph)\n",
    "\n",
    "saver.restore(sess, 'models/' + model_name)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    # Training on train set\n",
    "    for i in tqdm(range(num_train_batches)):\n",
    "        batch_inp, batch_outp = data_loader.next_batch()\n",
    "        \n",
    "        input_dict = {encode_input[t]: batch_inp[t] for t in range(seq_length)}\n",
    "        input_dict.update({labels[t]: batch_outp[t] for t in range(seq_length)})\n",
    "        input_dict[keep_prob] = 1.0\n",
    "        \n",
    "        _, loss_val, summary = sess.run([train, loss, merged], feed_dict=input_dict)\n",
    "        train_losses.append(loss_val)\n",
    "        \n",
    "        summary_writer.add_summary(summary, step)\n",
    "        step += 1\n",
    "        \n",
    "    # Testing on valid set\n",
    "    for i in tqbatch_outpdm(range(num_valid_batches)):\n",
    "        batch_inp, batch_outp = data_loader.next_batch(data_type='valid')\n",
    "        \n",
    "        input_dict = {encode_input[t]: batch_inp[t] for t in range(seq_length)}\n",
    "        input_dict.update({labels[t]: batch_outp[t] for t in range(seq_length)})\n",
    "        input_dict[keep_prob] = 1.0\n",
    "        \n",
    "        loss_val = sess.run(loss, feed_dict=input_dict)\n",
    "        valid_losses.append(loss_val)\n",
    "        \n",
    "    log_txt = \"Epoch: \" + str(epoch) + \" Steps: \" + str(step) + \\\n",
    "            \" train_loss: \" + str(round(np.mean(train_losses), 4)) + '+' + str(round(np.std(train_losses), 2)) + \\\n",
    "            \" valid_loss: \" + str(round(np.mean(valid_losses), 4)) + '+' + str(round(np.std(valid_losses), 2)) \n",
    "    print log_txt\n",
    "    \n",
    "    f = open('log.txt', 'a')\n",
    "    f.write(log_txt + '\\n')\n",
    "    f.close()\n",
    "    \n",
    "    saver.save(sess, 'models/' + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "\n",
    "# Testing on test set\n",
    "for i in tqdm(range(num_test_batches)):\n",
    "    batch_q1, batch_q2 = data_loader.next_batch(data_type='test')\n",
    "\n",
    "    input_dict = {encode_input[t]: batch_q1[t] for t in range(seq_length)}\n",
    "    input_dict.update({labels[t]: batch_q2[t] for t in range(seq_length)})\n",
    "    input_dict[keep_prob] = 1.0\n",
    "\n",
    "    loss_val = sess.run(loss, feed_dict=input_dict)\n",
    "    test_losses.append(loss_val)\n",
    "\n",
    "\n",
    "log_txt = \"Test_loss: \" + str(round(np.mean(test_losses), 4)) + '+' + str(round(np.std(test_losses), 2)) \n",
    "print log_txt\n",
    "\n",
    "f = open('log.txt', 'a')\n",
    "f.write(log_txt + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting output and analysing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get batch data like batch_q1, batch_q2\n",
    "2. get outputs from decode_outputs_test and argmax\n",
    "3. use swapaxes to change it to [batch_size, seq_length] for all batch_q1, batch_q2, output\n",
    "4. use data_loader.idx2word to generate words for all batch_q1, batch_q2, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_inp, batch_outp = data_loader.next_batch()\n",
    "\n",
    "input_dict = {encode_input[t]: batch_q1[t] for t in range(seq_length)}\n",
    "input_dict.update({labels[t]: batch_q2[t] for t in range(seq_length)})\n",
    "input_dict[keep_prob] = 1.0\n",
    "\n",
    "loss_val, outputs = sess.run([loss, decode_outputs_test], feed_dict = input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded_outputs = np.array(outputs).transpose([1,0,2])\n",
    "decoded_outputs = np.argmax(outputs, axis = 2)\n",
    "\n",
    "inps = np.swapaxes(batch_inp, 0, 1)\n",
    "outps = np.swapaxes(batch_outp, 0, 1)\n",
    "gens = np.swapaxes(decoded_outputs, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = random.randint(0, batch_size-1)\n",
    "\n",
    "inp = ''.join([data_loader.idx2word[x] for x in inps[index] if x != 14681]).replace('\\xe2\\x96\\x81', ' ')\n",
    "outp = ''.join([data_loader.idx2word[x] for x in outps[index] if x != 14681]).replace('\\xe2\\x96\\x81', ' ')\n",
    "gen = ''.join([data_loader.idx2word[x] for x in gens[index] if x != 14681]).replace('\\xe2\\x96\\x81', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " palvalAni taqAgAni girikUwAni sarvaSaH\n",
      " palvala taqAga giri kUwa sarvaSas\n"
     ]
    }
   ],
   "source": [
    "print inp\n",
    "print outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inin vAtakaP vAtakaPstIrstIrAMsiAMsiAMsi kapola kapola kapola kapola kapola pArAvat t t deva deva deva deva deva deva deva deva deva turaga turagaharatharat Slezm Slezm Slezm SlezmmArga naktamArga nakta purastAt purastAt anyat evEt Bog Pull Pull Pull pARinAantampramAdsaMDAnpramAd unmuKa gurur anantA niSci pravartaka pravartakaAtmanA nirIkz baBUvAitEH praSastamity vIr tav jagAm udAharaRa cEntak viparyay kapitTagrahraSmiituM harmyapede gfDyazwyAhv Bruku saMnyAsa kfzRA Sap kftaHgam prastuharzev cEnvaRatak carAcar niDan vy trikawu trikawu jitendriyaH samIray visaMjYa nanu mfz vidArI biBSUla rOpykOtuk arjanavahanvahan niHsf niHsf maNgaly vanaspatiAnyW samAkulavfdD daSaBirmuKI prabAD prabADvAditrnAsiknAsik prahAranAsiknAsik jImUta prahAraapariapari samAviS baBUvA samutpad gomaya yazwI AcCidayecC vEaparyant lehgacCAmisatkft AjYApay AjYApayIkftIkftpramAd prayAg prAR prAR\n"
     ]
    }
   ],
   "source": [
    "print gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputting the previous word for decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_inp, batch_outp = data_loader.next_batch()\n",
    "\n",
    "input_dict = {encode_input[t]: batch_q1[t] for t in range(seq_length)}\n",
    "input_dict.update({labels[t]: batch_q2[t] for t in range(seq_length)})\n",
    "input_dict[keep_prob] = 1.0\n",
    "\n",
    "#loss_val, outputs = sess.run([loss, decode_outputs_test], feed_dict = input_dict)\n",
    "loss_val, outputs = sess.run([loss, decode_outputs], feed_dict = input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded_outputs = np.array(outputs).transpose([1,0,2])\n",
    "decoded_outputs = np.argmax(outputs, axis = 2)\n",
    "\n",
    "inps = np.swapaxes(batch_inp, 0, 1)\n",
    "outps = np.swapaxes(batch_outp, 0, 1)\n",
    "gens = np.swapaxes(decoded_outputs, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = random.randint(0, batch_size-1)\n",
    "\n",
    "inp = ''.join([data_loader.idx2word[x] for x in inps[index] if x != 14681]).replace('\\xe2\\x96\\x81', ' ')\n",
    "outp = ''.join([data_loader.idx2word[x] for x in outps[index] if x != 14681]).replace('\\xe2\\x96\\x81', ' ')\n",
    "gen = ''.join([data_loader.idx2word[x] for x in gens[index] if x != 14681]).replace('\\xe2\\x96\\x81', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yaTAhaM rAGavAd anyaM manasApi na cintaye\n",
      " yaTA mad rAGava anya manas api na cintay\n"
     ]
    }
   ],
   "source": [
    "print inp\n",
    "print outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pariuddiSyaSizwSizwSizw jA jA jA asatyazwuzwu enad enad enad enad enad enad enad purIza ambA trilocana trilocana trilocana trilocana trilocana trilocana trilocana tuzw tuzw tuzw tuzw tuzw tuzw tuzw tuzw tuzw tuzw tuzw tuzw tuzw tuzw tuzw tuzw tuzw tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH tEH nirAlamba nirAlambaRo nirAlamba\n"
     ]
    }
   ],
   "source": [
    "print gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
